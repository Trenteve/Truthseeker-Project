{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Truthseeker Project\n",
    "Trent Everard\n",
    "\n",
    "CS 497\n",
    "\n",
    "11/22/24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/Truth_Seeker_Model_Dataset.csv\")\n",
    "df.shape\n",
    "# Filter out 'NO MAJORITY'\n",
    "df = df[~df['5_label_majority_answer'].isin(['NO MAJORITY'])]\n",
    "\n",
    "# Reset index after filtering\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statements = df['statement']\n",
    "groups = df['statement']\n",
    "\n",
    "gss = GroupShuffleSplit(test_size=0.2, n_splits=1, random_state=42)\n",
    "train_idx, test_idx = next(gss.split(df, groups=groups))\n",
    "\n",
    "train_df = df.iloc[train_idx]\n",
    "test_df = df.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Class Distribution:\n",
      "5_label_majority_answer\n",
      "Agree              43463\n",
      "Mostly Agree       42720\n",
      "Mostly Disagree     2308\n",
      "Disagree             432\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test Set Class Distribution:\n",
      "5_label_majority_answer\n",
      "Mostly Agree       11086\n",
      "Agree              10872\n",
      "Mostly Disagree      583\n",
      "Disagree             129\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Training set class distribution\n",
    "train_class_counts = train_df['5_label_majority_answer'].value_counts()\n",
    "print(\"Training Set Class Distribution:\")\n",
    "print(train_class_counts)\n",
    "\n",
    "# Test set class distribution\n",
    "test_class_counts = test_df['5_label_majority_answer'].value_counts()\n",
    "print(\"\\nTest Set Class Distribution:\")\n",
    "print(test_class_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the labels\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(train_df['5_label_majority_answer'])\n",
    "\n",
    "train_labels = label_encoder.transform(train_df['5_label_majority_answer'])\n",
    "test_labels = label_encoder.transform(test_df['5_label_majority_answer'])\n",
    "\n",
    "# Initialize the LabelBinarizer for one-hot encoding\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(train_labels)\n",
    "\n",
    "# Transform labels\n",
    "train_labels_encoded = lb.transform(train_labels)\n",
    "test_labels_encoded = lb.transform(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_480744/634348527.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['2class_label'] = train_df['5_label_majority_answer'].map(label_mapping)\n",
      "/tmp/ipykernel_480744/634348527.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['2class_label'] = test_df['5_label_majority_answer'].map(label_mapping)\n"
     ]
    }
   ],
   "source": [
    "# Map labels to two classes\n",
    "label_mapping = {\n",
    "    'Agree': 'Agree',\n",
    "    'Mostly Agree': 'Agree',\n",
    "    'Disagree': 'Disagree',\n",
    "    'Mostly Disagree': 'Disagree'\n",
    "}\n",
    "\n",
    "train_df['2class_label'] = train_df['5_label_majority_answer'].map(label_mapping)\n",
    "test_df['2class_label'] = test_df['5_label_majority_answer'].map(label_mapping)\n",
    "\n",
    "label_encoder_2class = LabelEncoder()\n",
    "label_encoder_2class.fit(train_df['2class_label'])\n",
    "\n",
    "train_labels_2class = label_encoder_2class.transform(train_df['2class_label'])\n",
    "test_labels_2class = label_encoder_2class.transform(test_df['2class_label'])\n",
    "\n",
    "# Encode labels\n",
    "lb_2class = LabelBinarizer()\n",
    "lb_2class.fit(train_labels_2class)\n",
    "\n",
    "train_labels_2class_encoded = lb_2class.transform(train_labels_2class)\n",
    "test_labels_2class_encoded = lb_2class.transform(test_labels_2class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Encoder Classes: ['Agree' 'Disagree' 'Mostly Agree' 'Mostly Disagree']\n",
      "\n",
      "Label Mapping:\n",
      "0: Agree\n",
      "1: Disagree\n",
      "2: Mostly Agree\n",
      "3: Mostly Disagree\n",
      "Sample train labels (4-class): [2 0 2 0 0]\n",
      "Sample test labels (4-class): [2 2 0 2 2]\n",
      "Sample train labels (2-class): [0 0 0 0 0]\n",
      "Sample test labels (2-class): [0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(\n",
    "            texts.tolist(),\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=512\n",
    "        )\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        # For multi-class classification, labels can be integers\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# For 4-class problem\n",
    "train_texts = train_df['statement']\n",
    "test_texts = test_df['statement']\n",
    "\n",
    "train_dataset = TextDataset(train_texts, train_labels)\n",
    "test_dataset = TextDataset(test_texts, test_labels)\n",
    "\n",
    "# For 2-class problem\n",
    "train_dataset_2class = TextDataset(train_texts, train_labels_2class)\n",
    "test_dataset_2class = TextDataset(test_texts, test_labels_2class)\n",
    "\n",
    "print(\"Label Encoder Classes:\", label_encoder.classes_)\n",
    "print(\"\\nLabel Mapping:\")\n",
    "for idx, label in enumerate(label_encoder.classes_):\n",
    "    print(f\"{idx}: {label}\")\n",
    "print(\"Sample train labels (4-class):\", train_labels[:5])\n",
    "print(\"Sample test labels (4-class):\", test_labels[:5])\n",
    "print(\"Sample train labels (2-class):\", train_labels_2class[:5])\n",
    "print(\"Sample test labels (2-class):\", test_labels_2class[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine texts and labels into a DataFrame for resampling\n",
    "train_data = pd.DataFrame({\n",
    "    'statement': train_df['statement'],\n",
    "    'label': train_labels\n",
    "})\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(\n",
    "    train_data[['statement']], train_data['label']\n",
    ")\n",
    "\n",
    "train_texts_resampled = X_resampled['statement']\n",
    "train_labels_resampled = y_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model for 4-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/nbuser/demo/bert/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_480744/3653647465.py:29: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch in tqdm(train_loader):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d7bb81194484a39a7868920fac225df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10866 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.4778\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9469eb7e34f456c906d1e04936338a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10866 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Training Loss: 0.4538\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce698fc0113c4689ac8233eeeae80a10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10866 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Training Loss: 0.4507\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd3825ea894543cfb62b2f01c4b09ac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10866 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Training Loss: 0.4478\n"
     ]
    }
   ],
   "source": [
    "# Use GPU if available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)\n",
    "model.to(device)\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TextDataset(train_texts_resampled, train_labels_resampled)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_labels),\n",
    "    y=train_labels\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# Loss Function\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# # Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Training loop\n",
    "epochs = 4\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        loss = loss_fn(logits, labels)\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}, Training Loss: {avg_train_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model for 2-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_480744/1913818282.py:16: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch in tqdm(train_loader):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d54a62c17bd4455da15f24f9ef52bd84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10866 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.4440\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model_2class = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "model_2class.to(device)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader_2class = DataLoader(train_dataset_2class, batch_size=32, shuffle=True)\n",
    "test_loader_2class = DataLoader(test_dataset_2class, batch_size=32)\n",
    "\n",
    "# Optimizer\n",
    "optimizer_2class = AdamW(model_2class.parameters(), lr=2e-5)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        optimizer_2class.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        loss = loss_fn(logits, labels)\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer_2class.step()\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}, Training Loss: {avg_train_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model for 4-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_480744/1594193361.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch in tqdm(test_loader):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fa2b4a2c15c43c1896619c506b6ec6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1417 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-Class Classification Accuracy: 0.1368\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "          Agree       0.49      0.24      0.32     10872\n",
      "       Disagree       0.01      0.40      0.01       129\n",
      "   Mostly Agree       0.46      0.02      0.03     11086\n",
      "Mostly Disagree       0.03      0.43      0.05       583\n",
      "\n",
      "       accuracy                           0.14     22670\n",
      "      macro avg       0.25      0.27      0.10     22670\n",
      "   weighted avg       0.46      0.14      0.17     22670\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, axis=1)\n",
    "        \n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(f'4-Class Classification Accuracy: {accuracy:.4f}')\n",
    "\n",
    "target_names = [label_encoder.classes_[idx] for idx in range(len(label_encoder.classes_))]\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(true_labels, predictions, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model for 2-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_480744/2728220375.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch in tqdm(test_loader_2class):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7a37c7f921d43c8af73514076cfbfdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/709 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-Class Classification Accuracy: 0.1354\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Agree       0.98      0.11      0.20     21958\n",
      "    Disagree       0.03      0.91      0.06       712\n",
      "\n",
      "    accuracy                           0.14     22670\n",
      "   macro avg       0.50      0.51      0.13     22670\n",
      "weighted avg       0.95      0.14      0.19     22670\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_2class.eval()\n",
    "predictions_2class = []\n",
    "true_labels_2class = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader_2class):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model_2class(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, axis=1)\n",
    "        \n",
    "        predictions_2class.extend(preds.cpu().numpy())\n",
    "        true_labels_2class.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_2class = accuracy_score(true_labels_2class, predictions_2class)\n",
    "print(f'2-Class Classification Accuracy: {accuracy_2class:.4f}')\n",
    "\n",
    "# If classes are not strings, convert them\n",
    "target_names = [str(cls) for cls in label_encoder_2class.classes_]\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(true_labels_2class, predictions_2class, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
